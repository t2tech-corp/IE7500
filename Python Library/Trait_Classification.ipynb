{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db172d75-aa8c-4e9c-ad97-e18b12de000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules  \n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import nltk  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "import re  \n",
    "from tqdm import tqdm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3012f4fb-9224-47d3-82b0-b2854b5abca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tterr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tterr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tterr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources  \n",
    "nltk.download('stopwords')  \n",
    "nltk.download('punkt')  \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b3931dd-02e3-4b89-b3f8-e2fb58ca012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded movie_lines.tsv, shape: (304543, 5)\n",
      "  lineID characterID movieID characterName          text\n",
      "0  L1045          u0      m0        BIANCA  They do not!\n",
      "1  L1044          u2      m0       CAMERON   They do to!\n"
     ]
    }
   ],
   "source": [
    " # Load datasets from TSV files using updated on_bad_lines parameter  \n",
    "movie_lines = pd.read_csv('data/movie_lines.tsv', sep='\\t', header=None,  \n",
    "                       names=['lineID', 'characterID', 'movieID', 'characterName', 'text'],  \n",
    "                       encoding='utf-8', on_bad_lines='skip', quoting=3) \n",
    "\n",
    "print(\"Loaded movie_lines.tsv, shape:\", movie_lines.shape)  \n",
    "print(movie_lines.head(2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604a9d2c-5921-4256-9828-d3df20607963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded movie_characters_metadata.tsv, shape: (9034, 6)\n",
      "  characterID characterName movieID                  movieTitle gender  \\\n",
      "0          u0        BIANCA      m0  10 things i hate about you      f   \n",
      "1          u1         BRUCE      m0  10 things i hate about you      ?   \n",
      "\n",
      "  position  \n",
      "0        4  \n",
      "1        ?  \n"
     ]
    }
   ],
   "source": [
    "movie_characters = pd.read_csv('data/movie_characters_metadata.tsv', sep='\\t', header=None,  \n",
    "                                names=['characterID', 'characterName', 'movieID', 'movieTitle', 'gender', 'position'],  \n",
    "                                encoding='utf-8', on_bad_lines='skip', quoting=3)  \n",
    "\n",
    "print(\"Loaded movie_characters_metadata.tsv, shape:\", movie_characters.shape)  \n",
    "print(movie_characters.head(2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9401ce18-8909-4ba7-890f-a754b49a9e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded movie_titles_metadata.tsv, shape: (617, 6)\n",
      "  movieID                  movieTitle movieYear  IMDB_rating  IMDB_votes  \\\n",
      "0      m0  10 things i hate about you      1999          6.9       62847   \n",
      "1      m1  1492: conquest of paradise      1992          6.2       10421   \n",
      "\n",
      "                                        genres  \n",
      "0                         ['comedy' 'romance']  \n",
      "1  ['adventure' 'biography' 'drama' 'history']  \n"
     ]
    }
   ],
   "source": [
    "movie_titles = pd.read_csv('data/movie_titles_metadata.tsv', sep='\\t', header=None,  \n",
    "                            names=['movieID', 'movieTitle', 'movieYear', 'IMDB_rating', 'IMDB_votes', 'genres'],  \n",
    "                            encoding='utf-8', on_bad_lines='skip', quoting=3)  \n",
    "\n",
    "print(\"Loaded movie_titles_metadata.tsv, shape:\", movie_titles.shape)  \n",
    "print(movie_titles.head(2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5de4d6a-91ed-49ab-ac6a-737f131aeb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined character dialogue (sample):\n",
      "  characterID                                  combined_dialogue\n",
      "0          u0  They do not! I hope so. Let's go. Okay -- you'...\n",
      "1          u1  Just sent 'em through. Never Didn't have you p...\n",
      "2         u10  Absolutely not. Your daughters went to the pro...\n",
      "3        u100  She died in her sleep three days ago.  It was ...\n",
      "4       u1000  Yeah and I'm gonna be right back at it tomorro...\n"
     ]
    }
   ],
   "source": [
    "# Combine dialogue per character from movie_lines  \n",
    "# Group by characterID so we have a single combined dialogue per character  \n",
    "character_dialogue = movie_lines.groupby('characterID')['text'].apply(lambda x: ' '.join(x.astype(str))).reset_index()  \n",
    "character_dialogue.columns = ['characterID', 'combined_dialogue']  \n",
    "\n",
    "print(\"Combined character dialogue (sample):\")  \n",
    "print(character_dialogue.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb9cf9dd-89e3-4f02-969c-c25d224fa86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing combined dialogue...\n",
      "Sample processed text:\n",
      "  characterID                                     processed_text\n",
      "0          u0  hope let okay gon need learn lie kidding know ...\n",
      "1          u1  sent never pegged gigglepuss fan little pre te...\n",
      "2         u10  absolutely daughter went prom great time honey...\n",
      "3        u100  died sleep three day ago paper tom dead callin...\n",
      "4       u1000  yeah gon right back tomorrow tonight gon sit f...\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dialogue text to clean it.  \n",
    "# Define a function to preprocess text: lowercasing, remove punctuation, tokenize, remove stopwords, lemmatize.  \n",
    "stop_words = set(stopwords.words('english'))  \n",
    "lemmatizer = WordNetLemmatizer()  \n",
    "\n",
    "def preprocess_text(text):  \n",
    "     # Lowercase the text  \n",
    "     text = text.lower()  \n",
    "     # Remove non-alphanumeric characters (keep spaces)  \n",
    "     text = re.sub(r'[^a-z0-9\\s]', ' ', text)  \n",
    "     # Tokenize the text  \n",
    "     tokens = nltk.word_tokenize(text)  \n",
    "     # Remove stopwords and short tokens  \n",
    "     tokens = [word for word in tokens if word not in stop_words and len(word) > 2]  \n",
    "     # Lemmatize tokens  \n",
    "     tokens = [lemmatizer.lemmatize(word) for word in tokens]  \n",
    "     # Return processed text  \n",
    "     return ' '.join(tokens)  \n",
    "\n",
    "print(\"Preprocessing combined dialogue...\")  \n",
    "character_dialogue['processed_text'] = character_dialogue['combined_dialogue'].apply(preprocess_text)  \n",
    "\n",
    "print(\"Sample processed text:\")  \n",
    "print(character_dialogue[['characterID', 'processed_text']].head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8c4d7f9-bd47-445e-bc88-693a0deba415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we move to trait classification using a pre-trained model from Hugging Face.  \n",
    "\n",
    "from transformers import pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b24ad03c-6057-4d92-b65c-9f9cb5289c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define candidate traits to check from each character's dialogue.  \n",
    "candidate_traits = [  \n",
    " \"friendly\", \"hostile\",  \n",
    " \"introverted\", \"extroverted\",  \n",
    " \"intelligent\", \"naive\",  \n",
    " \"courageous\", \"cowardly\",  \n",
    " \"compassionate\", \"ruthless\",  \n",
    " \"funny\", \"serious\"  \n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af5b86f6-71e6-49fc-8391-f47a09729bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fbfaaf190b495387d0046944ba62ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tterr\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tterr\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tterr\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebc8f5e45bd44ec80e925409fe56c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c587c4a6147f49cba40596000eade85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c6fdffc3954752887e18597bde32d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45b74cb4e0a4975acdfdeb59754412b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f195aa35b794492a69864a6dab8a9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create the zero-shot classification pipeline using the model 'facebook/bart-large-mnli'  \n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "038024c2-eb5b-486d-9390-4ade47785adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying traits:   0%|          | 10/9034 [00:07<2:18:03,  1.09it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Classifying traits: 100%|██████████| 9034/9034 [1:29:10<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use the classifier on each character's combined dialogue text.  \n",
    "# We'll create a list of dictionaries with results.  \n",
    "trait_predictions = []  \n",
    "\n",
    "for idx, row in tqdm(character_dialogue.iterrows(), total=len(character_dialogue), desc=\"Classifying traits\"):  \n",
    " text = row['combined_dialogue']  \n",
    " # Run zero-shot classification on the dialogue text with candidate traits.  \n",
    " result = classifier(text, candidate_traits)  \n",
    " dominant_trait = result['labels'][0]  \n",
    " trait_predictions.append({  \n",
    "     'characterID': row['characterID'],  \n",
    "     'combined_dialogue': text,  \n",
    "     'dominant_trait': dominant_trait,  \n",
    "     'trait_scores': result['scores']  \n",
    " })  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e34df719-52e8-4e0f-85e9-639326c3f5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trait classification complete. Sample results:\n",
      "  characterID                                  combined_dialogue  \\\n",
      "0          u0  They do not! I hope so. Let's go. Okay -- you'...   \n",
      "1          u1  Just sent 'em through. Never Didn't have you p...   \n",
      "2         u10  Absolutely not. Your daughters went to the pro...   \n",
      "3        u100  She died in her sleep three days ago.  It was ...   \n",
      "4       u1000  Yeah and I'm gonna be right back at it tomorro...   \n",
      "\n",
      "  dominant_trait                                       trait_scores  \n",
      "0    introverted  [0.1399690806865692, 0.10658851265907288, 0.09...  \n",
      "1          funny  [0.5015628337860107, 0.17719238996505737, 0.16...  \n",
      "2        hostile  [0.1995924562215805, 0.18560273945331573, 0.13...  \n",
      "3        hostile  [0.29701340198516846, 0.2610812783241272, 0.07...  \n",
      "4        serious  [0.25978192687034607, 0.12038370221853256, 0.0...  \n"
     ]
    }
   ],
   "source": [
    "# Convert the list to a DataFrame  \n",
    "traits_df = pd.DataFrame(trait_predictions) \n",
    "\n",
    "print(\"Trait classification complete. Sample results:\")  \n",
    "print(traits_df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d15fb179-41d0-4ca0-b55b-7681dfed4f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged character_interest (movie_characters with trait classification):\n",
      "  characterID characterName movieID                  movieTitle gender  \\\n",
      "0          u0        BIANCA      m0  10 things i hate about you      f   \n",
      "1          u1         BRUCE      m0  10 things i hate about you      ?   \n",
      "2          u2       CAMERON      m0  10 things i hate about you      m   \n",
      "3          u3      CHASTITY      m0  10 things i hate about you      ?   \n",
      "4          u4          JOEY      m0  10 things i hate about you      m   \n",
      "\n",
      "  position dominant_trait  \n",
      "0        4    introverted  \n",
      "1        ?          funny  \n",
      "2        3    extroverted  \n",
      "3        ?    extroverted  \n",
      "4        6        serious  \n"
     ]
    }
   ],
   "source": [
    "# Merge the trait classification results with movie characters metadata to form character_interest  \n",
    "character_interest = pd.merge(movie_characters, traits_df[['characterID', 'dominant_trait']], on='characterID', how='left')  \n",
    "\n",
    "print(\"Merged character_interest (movie_characters with trait classification):\")  \n",
    "print(character_interest.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39b4bb6d-160f-4613-a7da-1b939c405363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character interest DataFrame (merged):\n",
      "  characterID characterName movieID                  movieTitle gender  \\\n",
      "0          u0        BIANCA      m0  10 things i hate about you      f   \n",
      "1          u1         BRUCE      m0  10 things i hate about you      ?   \n",
      "2          u2       CAMERON      m0  10 things i hate about you      m   \n",
      "3          u3      CHASTITY      m0  10 things i hate about you      ?   \n",
      "4          u4          JOEY      m0  10 things i hate about you      m   \n",
      "\n",
      "  position dominant_trait            movieTitle_movie  \n",
      "0        4    introverted  10 things i hate about you  \n",
      "1        ?          funny  10 things i hate about you  \n",
      "2        3    extroverted  10 things i hate about you  \n",
      "3        ?    extroverted  10 things i hate about you  \n",
      "4        6        serious  10 things i hate about you  \n"
     ]
    }
   ],
   "source": [
    "# Merge with movie_titles metadata to include movie title details  \n",
    "character_interest = pd.merge(character_interest, movie_titles[['movieID','movieTitle']], on='movieID', how='left', suffixes=('', '_movie'))  \n",
    "\n",
    "print(\"Character interest DataFrame (merged):\")  \n",
    "print(character_interest.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec3a488f-567a-46a0-af4d-dcc76d6cd318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character trait classification saved to character_trait_classification.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the final classification results to a CSV file  \n",
    "output_filename = 'character_trait_classification.csv'  \n",
    "character_interest.to_csv(output_filename, index=False)  \n",
    "\n",
    "print(\"Character trait classification saved to\", output_filename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797b5c8-480f-4def-8303-e4c0031ee0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
